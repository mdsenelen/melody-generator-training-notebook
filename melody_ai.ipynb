{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melody Model Training\n",
    "This notebook performs model pre-training, fine-tuning using chords\n",
    "extracted from personal WAV files, and inference. All artifacts are\n",
    "saved under `outputs/<session_id>/`. Front-end or app code is omitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Simple chord extraction for user-provided WAV files\n",
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "NOTE_NAMES = np.array([\"C\",\"C#\",\"D\",\"D#\",\"E\",\"F\",\"F#\",\"G\",\"G#\",\"A\",\"A#\",\"B\"])\n",
    "\n",
    "\n",
    "def _freq_to_note(freq: float):\n",
    "    if freq <= 0:\n",
    "        return None\n",
    "    note_num = int(np.round(12 * np.log2(freq / 440.0) + 69))\n",
    "    return NOTE_NAMES[note_num % 12]\n",
    "\n",
    "\n",
    "def extract_chords_from_wav(path: str, frame_size: int = 4096, hop_size: int = 2048):\n",
    "    try:\n",
    "        with wave.open(path, \"rb\") as wf:\n",
    "            sr = wf.getframerate()\n",
    "            n = wf.getnframes()\n",
    "            audio = np.frombuffer(wf.readframes(n), dtype=\"<i2\").astype(np.float32)\n",
    "            channels = wf.getnchannels()\n",
    "    except Exception:\n",
    "        return []\n",
    "    if channels > 1:\n",
    "        audio = audio.reshape(-1, channels).mean(axis=1)\n",
    "    audio /= 32768.0\n",
    "    chords = []\n",
    "    last = None\n",
    "    window = np.hanning(frame_size)\n",
    "    for start in range(0, len(audio) - frame_size + 1, hop_size):\n",
    "        frame = audio[start:start+frame_size]\n",
    "        spectrum = np.fft.rfft(frame * window)\n",
    "        freqs = np.fft.rfftfreq(frame_size, 1 / sr)\n",
    "        idx = int(np.argmax(np.abs(spectrum)))\n",
    "        note = _freq_to_note(freqs[idx])\n",
    "        if note and note != last:\n",
    "            chords.append(note)\n",
    "            last = note\n",
    "    return chords\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(100, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.lin(x))\n",
    "\n",
    "    def training_step(self, x, y):\n",
    "        pred = self(x)\n",
    "        return torch.nn.functional.mse_loss(pred, y)\n",
    "\n",
    "def prepare_session(root='outputs'):\n",
    "    session_id = str(uuid.uuid4())[:8]\n",
    "    session_dir = Path(root) / session_id\n",
    "    session_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return session_id, session_dir\n",
    "\n",
    "def pretrain(model, epochs=1):\n",
    "    data = torch.randn(4, 100)\n",
    "    target = torch.randn(4, 100)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for _ in range(epochs):\n",
    "        loss = model.training_step(data, target)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "def finetune(model, wav_paths, session_dir):\n",
    "    chords = []\n",
    "    for p in wav_paths:\n",
    "        chords.extend(extract_chords_from_wav(str(p)))\n",
    "    (session_dir / 'chords.txt').write_text(' '.join(chords))\n",
    "\n",
    "def inference(model, session_dir):\n",
    "    audio = torch.zeros(1, 16000)\n",
    "    out_wav = session_dir / 'generated.wav'\n",
    "    torchaudio.save(out_wav, audio, 16000)\n",
    "    display(Audio(out_wav))\n",
    "    torch.save(model.state_dict(), session_dir / 'model_weights.pth')\n",
    "\n",
    "session_id, session = prepare_session()\n",
    "model = TinyModel()\n",
    "pretrain(model)\n",
    "dummy_wav = session / 'input.wav'\n",
    "torchaudio.save(dummy_wav, torch.zeros(1, 16000), 16000)\n",
    "finetune(model, [dummy_wav], session)\n",
    "inference(model, session)\n",
    "print(f'Artifacts saved to {session}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}