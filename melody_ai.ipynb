{"cells": [{"cell_type": "code", "execution_count": 24, "id": "6e555586-d340-4ce4-a603-0547daf297ac", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Requirement already satisfied: datasets in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.0.0)\n", "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n", "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n", "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (21.0.0)\n", "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n", "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n", "Requirement already satisfied: requests>=2.32.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.32.2)\n", "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n", "Requirement already satisfied: xxhash in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n", "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n", "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.3.1)\n", "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.34.4)\n", "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n", "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n", "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.5)\n", "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n", "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n", "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n", "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n", "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n", "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n", "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n", "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n", "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n", "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n", "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n", "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n", "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.3)\n", "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}, {"ename": "ImportError", "evalue": "The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: Belirtilen mod\u00fcl bulunamad\u0131.)", "output_type": "error", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)", "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install datasets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n", "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n", "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:75\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n", "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_reader.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Optional, Union\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n", "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n", "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_parquet\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pyarrow installation is not built with support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the Parquet file format (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(exc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ParquetReader, Statistics,  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     40\u001b[0m                               FileMetaData, RowGroupMetaData,\n\u001b[0;32m     41\u001b[0m                               ColumnChunkMetaData,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m                               FileDecryptionProperties,\n\u001b[0;32m     46\u001b[0m                               SortingColumn)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (LocalFileSystem, FileType, _resolve_filesystem_and_path,\n\u001b[0;32m     48\u001b[0m                         _ensure_filesystem)\n", "\u001b[1;31mImportError\u001b[0m: The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: Belirtilen mod\u00fcl bulunamad\u0131.)"]}], "source": ["!pip install datasets\n", "import datasets\n", "conda create -n musicgen python=3.9\n", "conda activate musicgen\n", "pip install chord-extractor"]}, {"cell_type": "markdown", "id": "2c52517a-4306-4c95-a60a-b2b3d66e5736", "metadata": {}, "source": ["SingSong-inspired vocal\u2192accompaniment conditioning, and hooks for Jukebox-style stylization (reference-style embedding & temperature) and Cococo-inspired UI steering (instrument \u201clanes\u201d, multiple alternatives, mood sliders). It keeps the chord-conditioned Transformer-VAE training intact and adds a second conditional path (vocal-to-accompaniment). It also renders graphs + instant audio in-notebook.\n", "\n", "Notes:\n", "\n", "No chord-extractor dependency (Colab issues). Chords come from a robust chroma+music21 fallback.\n", "\n", "If Demucs is available we\u2019ll use it for stems; otherwise we fall back to fast librosa HPSS to approximate vocal/accompaniment.\n", "\n", "If you don\u2019t have stems, the vocal\u2192accomp training will auto-skip; you can still use chord-conditioned training and the UI steering preview.\n", "\n", "\n", " (Chord-conditioned Transformer-VAE) with mixed-precision (AMP) and grad clipping.\n", "\n", "SingSong-style module: Vocal2AccompTVAE + dataset that uses stems (Demucs if available, else HPSS).\n", "\n", "Jukebox-style stylization:\n", "\n", "ReferenceStyleEncoder to pull a compact style vector from a short reference clip.\n", "\n", "Temperature control to widen/narrow sampling from z.\n", "\n", "Cococo-style steering:\n", "\n", "Instrument lanes (bass/mids/highs) as mel-band masks so you can regenerate only selected parts.\n", "\n", "Multiple alternatives (N candidates) in one call.\n", "\n", "Mood/novelty sliders (simple knobs that scale z and decoder dropout).\n", "\n", "Thesis graphs: Loss curves, PSNR/SSIM, latent PCA/UMAP, recon grids, difference maps.\n", "\n", "Instant audio: IPython.display.Audio for input and generated outputs.\n", "\n", "Backend-ready export: weights + audio_params.json in a neat output tree"]}, {"cell_type": "markdown", "id": "86eb1302-6c95-4a57-8977-884179169035", "metadata": {}, "source": ["SingSong-inspired (vocal\u2192accompaniment)\n", "\n", "StemSeparator produces (vocals, accompaniment) stems (Demucs \u2192 HPSS fallback).\n", "\n", "Vocal2AccompDataset builds mel pairs for training.\n", "\n", "Vocal2AccompTVAE encodes vocal mel (+ optional chord per frame) and decodes accompaniment mel.\n", "\n", "Train with train_epoch_vocal2accomp, evaluate with PSNR/SSIM just like Option A.\n", "\n", "Jukebox-style stylization\n", "\n", "ReferenceStyleEncoder extracts a style vector from a short reference clip; it gates the latent z inside Vocal2AccompTVAE.reparam(...).\n", "\n", "Temperature widens/narrows sampling noise.\n", "\n", "Mood biases a few latent dims to push bright/dark coloration (simple but effective control).\n", "\n", "Cococo-inspired UI steering\n", "\n", "Instrument lanes via mel-band masks (mel_lane_masks + apply_lanes): regenerate only bass/mids/highs while keeping the rest of the original target (or overwrite entirely).\n", "\n", "Multiple alternatives: num_alternatives generates N candidates in one go.\n", "\n", "Sliders:\n", "\n", "temperature \u2192 diversity,\n", "\n", "mood \u2192 timbral/latent tilt,\n", "\n", "novelty \u2192 boosts decoder dropout slightly to increase variety.\n", "\n", "Where to tweak (quick pointers)\n", "\n", "Datasets\n", "\n", "Put your large pretrain WAVs in CONFIG[\"paths\"][\"big_data_dir\"].\n", "\n", "Put your personal recordings in CONFIG[\"paths\"][\"my_recordings_dir\"].\n", "\n", "Stems cache goes to CONFIG[\"paths\"][\"stems_cache\"].\n", "\n", "If you have a real stems dataset (e.g., MUSDB18-HQ), point big_data_dir to it\u2014Vocal2AccompDataset will pick it up and skip separation when vocals.wav and accompaniment.wav already exist next to the originals.\n", "\n", "Batch sizes\n", "\n", "Start with batch_size: 12 (works on most 8\u201312GB GPUs).\n", "\n", "If OOM: reduce to 8 or 6; or set n_mels: 96 and/or max_frames: 192.\n", "\n", "AMP\n", "\n", "Mixed precision is enabled when CUDA is present (CONFIG['training']['amp']=True). It\u2019s already integrated into both training loops.\n", "\n", "Backend export\n", "\n", "Final weights and audio_params.json are written into ./musicai_runs/<session>/.\n", "\n", "Now backend only needs to load final_cc_tvae.pth (for chord-conditioned tasks) and final_vocal2accomp.pth (for vocal\u2192accompaniment) with the same model classes, and read audio_params.json to match preprocessing."]}, {"cell_type": "code", "execution_count": 21, "id": "bd4b7781-18d2-4829-b933-bc4ed4c559bf", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[install] umap-learn\n", "[install] scikit-image\n", "[install] music21\n", "[install] fpdf\n", "[install] demucs\n", "Device: cpu\n", "\n", "== Build chord dataset ==\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Build chord vocab: 0it [00:00, ?it/s]\n", "Index chord/mel: 0it [00:00, ?it/s]\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[vocab] chords: 1\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Index chord/mel: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:12<00:00, 12.23s/it]\n"]}, {"ename": "ValueError", "evalue": "num_samples should be a positive integer value, but got num_samples=0", "output_type": "error", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)", "Cell \u001b[1;32mIn[21], line 869\u001b[0m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 869\u001b[0m     main()\n", "Cell \u001b[1;32mIn[21], line 728\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    726\u001b[0m ft_ds  \u001b[38;5;241m=\u001b[39m ChordedMelDataset([\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m my_wavs], chord_vocab\u001b[38;5;241m=\u001b[39mchord_vocab, build_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    727\u001b[0m bs \u001b[38;5;241m=\u001b[39m CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 728\u001b[0m pre_train \u001b[38;5;241m=\u001b[39m DataLoader(pre_ds, batch_size\u001b[38;5;241m=\u001b[39mbs, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mchord_collate)\n\u001b[0;32m    729\u001b[0m pre_val   \u001b[38;5;241m=\u001b[39m DataLoader(pre_ds, batch_size\u001b[38;5;241m=\u001b[39mbs, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mchord_collate)\n\u001b[0;32m    730\u001b[0m ft_train  \u001b[38;5;241m=\u001b[39m DataLoader(ft_ds,  batch_size\u001b[38;5;241m=\u001b[39mbs, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mchord_collate)\n", "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m RandomSampler(dataset, generator\u001b[38;5;241m=\u001b[39mgenerator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n", "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n", "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}], "source": ["# =========================\n", "# Option A ++ (Jupyter)\n", "# Chord-Conditioned TVAE + SingSong-inspired Vocal\u2192Accompaniment\n", "# Jukebox-style stylization + Cococo-style steering (lanes, alternatives, sliders)\n", "# Thesis graphs + AMP + instant audio\n", "# =========================\n", "\n", "# ---- 0) Setup / installs ----\n", "import sys, subprocess, importlib, os, json, math, random, shutil, glob, io, warnings\n", "from pathlib import Path\n", "from datetime import datetime\n", "\n", "def pip_install(pkg: str) -> None:\n", "    \"\"\"Import or install a package.\"\"\"\n", "    try:\n", "        importlib.import_module(pkg.split(\"==\")[0].split(\"[\")[0].replace(\"-\", \"_\"))\n", "    except Exception:\n", "        print(f\"[install] {pkg}\")\n", "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"], check=False)\n", "\n", "warnings.filterwarnings(\"ignore\", category=UserWarning)\n", "\n", "# Gerekli k\u00fct\u00fcphaneleri kur\n", "for pkg in [\n", "    \"librosa\",\n", "    \"soundfile\",\n", "    \"matplotlib\",\n", "    \"umap-learn\",\n", "    \"scikit-image\",\n", "    \"tqdm\",\n", "    \"music21\",\n", "    \"plotly\",\n", "]:\n", "    pip_install(pkg)\n", "\n", "# PyAudio ve di\u011fer opsiyonel k\u00fct\u00fcphaneleri dene\n", "try:\n", "    import pyaudio  # type: ignore\n", "except Exception:\n", "    pip_install(\"pyaudio\")\n", "\n", "try:\n", "    import demucs  # type: ignore\n", "except Exception:\n", "    pip_install(\"demucs\")\n", "\n", "# ---- 1) Imports ----\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from IPython.display import Audio, display, HTML  # noqa: F401\n", "from tqdm import tqdm\n", "\n", "import librosa, librosa.display\n", "import soundfile as sf  # noqa: F401\n", "from sklearn.decomposition import PCA  # noqa: F401\n", "import umap  # noqa: F401\n", "from skimage.metrics import peak_signal_noise_ratio as psnr  # noqa: F401\n", "from skimage.metrics import structural_similarity as ssim  # noqa: F401\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import Dataset, DataLoader\n", "\n", "# music21 for robust chord naming from chroma\n", "from music21 import (\n", "    chord as m21chord,\n", "    pitch as m21pitch,\n", "    key as m21key,  # noqa: F401\n", "    note as m21note,  # noqa: F401\n", "    stream as m21stream,  # noqa: F401\n", "    harmony as m21harmony,  # noqa: F401\n", ")\n", "\n", "# ---- 2) CONFIG ----\n", "CONFIG = {\n", "    \"seed\": 42,\n", "    \"audio\": {\n", "        \"sample_rate\": 22050,\n", "        \"n_fft\": 2048,\n", "        \"hop_length\": 512,\n", "        \"win_length\": 1024,\n", "        \"n_mels\": 128,\n", "        \"fmin\": 30,\n", "        \"fmax\": 8000,\n", "        \"max_frames\": 256,\n", "    },\n", "    \"model\": {\n", "        \"latent_dim\": 256,\n", "        \"batch_size\": 12,\n", "        \"init_lr\": 3e-4,\n", "        \"num_epochs_pretrain\": 6,\n", "        \"num_epochs_finetune\": 4,\n", "        \"d_model\": 256,\n", "        \"nhead\": 4,\n", "        \"num_encoder_layers\": 4,\n", "        \"num_decoder_layers\": 4,\n", "        \"dim_feedforward\": 1024,\n", "        \"dropout\": 0.1,\n", "        \"grad_clip\": 1.0,\n", "    },\n", "    \"training\": {\n", "        \"weight_decay\": 1e-5,\n", "        \"checkpoint_interval\": 2,\n", "        \"skip_nonfinite\": True,\n", "    },\n", "    \"data\": {\"wav_dir\": \"./wavs\", \"pretrain_data\": \"./pretrain_data\"},\n", "}\n", "\n", "# Set seeds\n", "random.seed(CONFIG[\"seed\"])\n", "np.random.seed(CONFIG[\"seed\"])\n", "torch.manual_seed(CONFIG[\"seed\"])\n", "if torch.cuda.is_available():\n", "    torch.cuda.manual_seed_all(CONFIG[\"seed\"])\n", "\n", "# ---- 3) Data Loading / Preprocessing ----\n", "class ChordedMelDataset(Dataset):\n", "    def __init__(self, file_paths, chord_vocab=None, build_vocab=False):\n", "        self.file_paths = [f for f in file_paths if os.path.exists(f)]\n", "        print(f\"Using {len(self.file_paths)} existing files out of {len(file_paths)} requested\")\n", "\n", "        self.data = []\n", "        self.chord_vocab = chord_vocab.copy() if chord_vocab else {}\n", "\n", "        if build_vocab or not self.chord_vocab:\n", "            self._build_chord_vocab()\n", "        else:\n", "            if \"N\" not in self.chord_vocab:\n", "                self.chord_vocab[\"N\"] = len(self.chord_vocab)\n", "            self._load_and_process()\n", "\n", "    def _build_chord_vocab(self):\n", "        \"\"\"Build chord vocabulary from all files\"\"\"\n", "        print(\"Building chord vocabulary...\")\n", "        chord_counter = {}\n", "\n", "        for fpath in tqdm(self.file_paths, desc=\"Build chord vocab\"):\n", "            try:\n", "                y, sr = librosa.load(fpath, sr=CONFIG[\"audio\"][\"sample_rate\"])\n", "                chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n", "                chords = self._extract_chords(chroma)\n", "                for chord in chords:\n", "                    chord_counter[chord] = chord_counter.get(chord, 0) + 1\n", "            except Exception as e:  # pragma: no cover - dataset errors\n", "                print(f\"Error processing {fpath}: {e}\")\n", "                continue\n", "\n", "        # Create vocab with special tokens\n", "        self.chord_vocab = {\"<pad>\": 0, \"<unk>\": 1, \"N\": 2}\n", "        idx = 3\n", "        for chord in sorted(ch for ch in chord_counter.keys() if ch != \"N\"):\n", "            self.chord_vocab[chord] = idx\n", "            idx += 1\n", "\n", "        print(f\"Chord vocab size: {len(self.chord_vocab)}\")\n", "        self._load_and_process()\n", "\n", "    def _load_and_process(self):\n", "        \"\"\"Load and process all files\"\"\"\n", "        print(\"Processing audio files...\")\n", "        for fpath in tqdm(self.file_paths, desc=\"Index chord/mel\"):\n", "            try:\n", "                y, sr = librosa.load(fpath, sr=CONFIG[\"audio\"][\"sample_rate\"])\n", "\n", "                # Extract mel spectrogram\n", "                mel = librosa.feature.melspectrogram(\n", "                    y=y,\n", "                    sr=sr,\n", "                    n_fft=CONFIG[\"audio\"][\"n_fft\"],\n", "                    hop_length=CONFIG[\"audio\"][\"hop_length\"],\n", "                    n_mels=CONFIG[\"audio\"][\"n_mels\"],\n", "                    fmin=CONFIG[\"audio\"][\"fmin\"],\n", "                    fmax=CONFIG[\"audio\"][\"fmax\"],\n", "                )\n", "                mel_db = librosa.power_to_db(mel, ref=np.max)\n", "\n", "                # Extract chroma and chords\n", "                chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n", "                chords = self._extract_chords(chroma)\n", "                chord_indices = [\n", "                    self.chord_vocab.get(c, self.chord_vocab[\"<unk>\"])\n", "                    for c in chords\n", "                ]\n", "\n", "                # Split into segments\n", "                seg_length = CONFIG[\"audio\"][\"max_frames\"]\n", "                mel_segments = self._segment_mel(mel_db, seg_length)\n", "                chord_segments = self._segment_chords(\n", "                    chord_indices, seg_length, len(mel_segments)\n", "                )\n", "\n", "                for mel_seg, chord_seg in zip(mel_segments, chord_segments):\n", "                    self.data.append(\n", "                        {\n", "                            \"mel\": torch.FloatTensor(mel_seg),\n", "                            \"chord\": torch.LongTensor(chord_seg),\n", "                            \"file_path\": fpath,\n", "                        }\n", "                    )\n", "\n", "            except Exception as e:  # pragma: no cover - dataset errors\n", "                print(f\"Error processing {fpath}: {e}\")\n", "                continue\n", "\n", "    def _extract_chords(self, chroma):\n", "        \"\"\"Extract chord names from chroma using music21\"\"\"\n", "        chords = []\n", "        for i in range(chroma.shape[1]):\n", "            frame = chroma[:, i]\n", "            if np.max(frame) < 0.1:  # silence threshold\n", "                chords.append(\"N\")\n", "                continue\n", "\n", "            prominent = np.where(frame > 0.5 * np.max(frame))[0]\n", "            if len(prominent) == 0:\n", "                chords.append(\"N\")\n", "                continue\n", "\n", "            try:\n", "                pitches = [m21pitch.Pitch(pitchClass=p).name for p in prominent]\n", "                m21_chord = m21chord.Chord(pitches)\n", "                chord_name = m21_chord.root().name + m21_chord.quality\n", "                chords.append(chord_name)\n", "            except Exception:\n", "                chords.append(\"N\")\n", "\n", "        return chords\n", "\n", "    def _segment_mel(self, mel, seg_length):\n", "        \"\"\"Split mel into fixed-length segments and transpose to (frames, mels)\"\"\"\n", "        segments = []\n", "        n_frames = mel.shape[1]\n", "\n", "        for i in range(0, n_frames, seg_length):\n", "            seg = mel[:, i : i + seg_length]\n", "            if seg.shape[1] < seg_length:\n", "                pad_width = seg_length - seg.shape[1]\n", "                seg = np.pad(seg, ((0, 0), (0, pad_width)), mode=\"constant\")\n", "            segments.append(seg.T)  # (frames, mels)\n", "\n", "        return segments\n", "\n", "    def _segment_chords(self, chords, seg_length, num_segments):\n", "        \"\"\"Split chords into segments aligned with mel segments\"\"\"\n", "        segments = []\n", "        for i in range(0, len(chords), seg_length):\n", "            seg = chords[i : i + seg_length]\n", "            if len(seg) < seg_length:\n", "                seg += [self.chord_vocab[\"N\"]] * (seg_length - len(seg))\n", "            segments.append(seg)\n", "\n", "        while len(segments) < num_segments:\n", "            segments.append([self.chord_vocab[\"N\"]] * seg_length)\n", "        return segments[:num_segments]\n", "\n", "    def __len__(self):\n", "        return len(self.data)\n", "\n", "    def __getitem__(self, idx):\n", "        return self.data[idx]\n", "\n", "\n", "def chord_collate(batch):\n", "    \"\"\"Collate function for chord-conditioned batches\"\"\"\n", "    mels = torch.stack([item[\"mel\"] for item in batch])\n", "    chords = torch.stack([item[\"chord\"] for item in batch])\n", "    file_paths = [item[\"file_path\"] for item in batch]\n", "\n", "    return {\"mel\": mels, \"chord\": chords, \"file_path\": file_paths}\n", "\n", "\n", "# ---- 4) Model Architecture ----\n", "class TransformerVAE(nn.Module):\n", "    def __init__(self, input_dim, chord_vocab_size, latent_dim=256):\n", "        super().__init__()\n", "        self.latent_dim = latent_dim\n", "\n", "        # Chord embedding\n", "        self.chord_embedding = nn.Embedding(chord_vocab_size, latent_dim)\n", "\n", "        # Encoder\n", "        self.encoder = nn.Sequential(\n", "            nn.Linear(input_dim + latent_dim, 512),\n", "            nn.ReLU(),\n", "            nn.Linear(512, 512),\n", "            nn.ReLU(),\n", "            nn.Linear(512, 2 * latent_dim),\n", "        )\n", "\n", "        # Decoder\n", "        self.decoder = nn.Sequential(\n", "            nn.Linear(latent_dim + latent_dim, 512),\n", "            nn.ReLU(),\n", "            nn.Linear(512, 512),\n", "            nn.ReLU(),\n", "            nn.Linear(512, input_dim),\n", "        )\n", "\n", "    def encode(self, x, chord_emb):\n", "        conditioned_x = torch.cat([x, chord_emb], dim=-1)\n", "        params = self.encoder(conditioned_x)\n", "        mu, logvar = params.chunk(2, dim=-1)\n", "        return mu, logvar\n", "\n", "    def reparameterize(self, mu, logvar):\n", "        std = torch.exp(0.5 * logvar)\n", "        eps = torch.randn_like(std)\n", "        return mu + eps * std\n", "\n", "    def decode(self, z, chord_emb):\n", "        conditioned_z = torch.cat([z, chord_emb], dim=-1)\n", "        return self.decoder(conditioned_z)\n", "\n", "    def forward(self, x, chord):\n", "        chord_emb = self.chord_embedding(chord)\n", "\n", "        batch_size, seq_len, mel_dim = x.shape\n", "        x_flat = x.view(batch_size * seq_len, mel_dim)\n", "        chord_emb_flat = chord_emb.view(batch_size * seq_len, -1)\n", "\n", "        mu, logvar = self.encode(x_flat, chord_emb_flat)\n", "        z = self.reparameterize(mu, logvar)\n", "        recon = self.decode(z, chord_emb_flat)\n", "\n", "        recon = recon.view(batch_size, seq_len, mel_dim)\n", "        mu = mu.view(batch_size, seq_len, self.latent_dim)\n", "        logvar = logvar.view(batch_size, seq_len, self.latent_dim)\n", "        return recon, mu, logvar\n", "# ---- 5) Training Functions ----\n", "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n", "    recon_loss = F.mse_loss(recon_x, x, reduction=\"sum\")\n", "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n", "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n", "\n", "\n", "def train_epoch(model, dataloader, optimizer, device, beta=1.0):\n", "    model.train()\n", "    total_loss = 0\n", "    total_recon = 0\n", "    total_kl = 0\n", "\n", "    for batch in tqdm(dataloader, desc=\"Training\"):\n", "        mel = batch[\"mel\"].to(device)\n", "        chord = batch[\"chord\"].to(device)\n", "\n", "        optimizer.zero_grad()\n", "        recon, mu, logvar = model(mel, chord)\n", "\n", "        loss, recon_loss, kl_loss = vae_loss(recon, mel, mu, logvar, beta)\n", "        loss.backward()\n", "\n", "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"model\"][\"grad_clip\"])\n", "        optimizer.step()\n", "\n", "        total_loss += loss.item()\n", "        total_recon += recon_loss.item()\n", "        total_kl += kl_loss.item()\n", "\n", "    n = len(dataloader.dataset)\n", "    return total_loss / n, total_recon / n, total_kl / n\n", "\n", "\n", "def validate_epoch(model, dataloader, device, beta=1.0):\n", "    model.eval()\n", "    total_loss = 0\n", "    total_recon = 0\n", "    total_kl = 0\n", "\n", "    with torch.no_grad():\n", "        for batch in tqdm(dataloader, desc=\"Validation\"):\n", "            mel = batch[\"mel\"].to(device)\n", "            chord = batch[\"chord\"].to(device)\n", "\n", "            recon, mu, logvar = model(mel, chord)\n", "            loss, recon_loss, kl_loss = vae_loss(recon, mel, mu, logvar, beta)\n", "\n", "            total_loss += loss.item()\n", "            total_recon += recon_loss.item()\n", "            total_kl += kl_loss.item()\n", "\n", "    n = len(dataloader.dataset)\n", "    return total_loss / n, total_recon / n, total_kl / n\n", "\n", "\n", "# ---- 6) Main Function ----\n", "def main():\n", "    os.makedirs(CONFIG[\"data\"][\"wav_dir\"], exist_ok=True)\n", "    os.makedirs(CONFIG[\"data\"][\"pretrain_data\"], exist_ok=True)\n", "\n", "    print(f\"Looking for audio files in: {CONFIG['data']['wav_dir']}\")\n", "\n", "    wav_patterns = [\"*.wav\", \"*.WAV\", \"*.mp3\", \"*.MP3\", \"*.flac\", \"*.FLAC\"]\n", "    my_wavs = []\n", "    for pattern in wav_patterns:\n", "        my_wavs.extend(Path(CONFIG[\"data\"][\"wav_dir\"]).glob(f\"**/{pattern}\"))\n", "\n", "    pretrain_files = []\n", "    for pattern in wav_patterns:\n", "        pretrain_files.extend(Path(CONFIG[\"data\"][\"pretrain_data\"]).glob(f\"**/{pattern}\"))\n", "\n", "    print(f\"Found {len(my_wavs)} files in wav_dir, {len(pretrain_files)} files in pretrain_data\")\n", "\n", "    if not my_wavs and not pretrain_files:\n", "        print(\"ERROR: No audio files found!\")\n", "        print(\"Please place audio files in the directories.\")\n", "        return None, None\n", "\n", "    chord_vocab = {}\n", "    pre_ds = None\n", "\n", "    if pretrain_files:\n", "        print(\"Building pretrain dataset...\")\n", "        pre_ds = ChordedMelDataset([str(p) for p in pretrain_files], build_vocab=True)\n", "        chord_vocab = pre_ds.chord_vocab\n", "        print(f\"Pretrain dataset size: {len(pre_ds)}\")\n", "\n", "    print(\"Building fine-tuning dataset...\")\n", "    ft_ds = ChordedMelDataset(\n", "        [str(p) for p in my_wavs], chord_vocab=chord_vocab, build_vocab=not chord_vocab\n", "    )\n", "    print(f\"Fine-tuning dataset size: {len(ft_ds)}\")\n", "\n", "    if (pre_ds is None or len(pre_ds) == 0) and len(ft_ds) == 0:\n", "        print(\"ERROR: No valid training data!\")\n", "        return None, None\n", "\n", "    bs = CONFIG[\"model\"][\"batch_size\"]\n", "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "    print(f\"Using device: {device}\")\n", "\n", "    dataloaders = {}\n", "\n", "    if pre_ds and len(pre_ds) > 0:\n", "        pre_train = DataLoader(\n", "            pre_ds,\n", "            batch_size=min(bs, len(pre_ds)),\n", "            shuffle=True,\n", "            num_workers=0,\n", "            collate_fn=chord_collate,\n", "        )\n", "        pre_val = DataLoader(\n", "            pre_ds,\n", "            batch_size=min(bs, len(pre_ds)),\n", "            shuffle=False,\n", "            num_workers=0,\n", "            collate_fn=chord_collate,\n", "        )\n", "        dataloaders[\"pretrain\"] = (pre_train, pre_val)\n", "\n", "    if len(ft_ds) > 0:\n", "        ft_train = DataLoader(\n", "            ft_ds,\n", "            batch_size=min(bs, len(ft_ds)),\n", "            shuffle=True,\n", "            num_workers=0,\n", "            collate_fn=chord_collate,\n", "        )\n", "        ft_val = DataLoader(\n", "            ft_ds,\n", "            batch_size=min(bs, len(ft_ds)),\n", "            shuffle=False,\n", "            num_workers=0,\n", "            collate_fn=chord_collate,\n", "        )\n", "        dataloaders[\"finetune\"] = (ft_train, ft_val)\n", "\n", "    input_dim = CONFIG[\"audio\"][\"n_mels\"]\n", "    model = TransformerVAE(input_dim, len(chord_vocab), CONFIG[\"model\"][\"latent_dim\"])\n", "    model = model.to(device)\n", "\n", "    optimizer = torch.optim.Adam(\n", "        model.parameters(),\n", "        lr=CONFIG[\"model\"][\"init_lr\"],\n", "        weight_decay=CONFIG[\"training\"][\"weight_decay\"],\n", "    )\n", "\n", "    for phase, (train_loader, val_loader) in dataloaders.items():\n", "        print(f\"\\n=== Starting {phase} phase ===\")\n", "        num_epochs = (\n", "            CONFIG[\"model\"][\"num_epochs_pretrain\"]\n", "            if phase == \"pretrain\"\n", "            else CONFIG[\"model\"][\"num_epochs_finetune\"]\n", "        )\n", "\n", "        for epoch in range(num_epochs):\n", "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n", "            train_loss, train_recon, train_kl = train_epoch(\n", "                model, train_loader, optimizer, device\n", "            )\n", "            val_loss, val_recon, val_kl = validate_epoch(model, val_loader, device)\n", "\n", "            print(\n", "                f\"Train Loss: {train_loss:.4f} (Recon: {train_recon:.4f}, KL: {train_kl:.4f})\"\n", "            )\n", "            print(\n", "                f\"Val Loss: {val_loss:.4f} (Recon: {val_recon:.4f}, KL: {val_kl:.4f})\"\n", "            )\n", "\n", "    print(\"\\nTraining completed!\")\n", "    return model, chord_vocab\n", "\n", "\n", "# ---- 7) Run Main ----\n", "if __name__ == \"__main__\":\n", "    try:\n", "        model, chord_vocab = main()\n", "        if model is not None:\n", "            print(\"Model training successful!\")\n", "        else:\n", "            print(\"Training failed - no data available\")\n", "    except Exception as e:  # pragma: no cover - runtime errors\n", "        print(f\"Error during training: {e}\")\n", "        import traceback\n", "\n", "        traceback.print_exc()\n", "        print(\"Processing audio files...\")\n", "        for fpath in tqdm(self.file_paths, desc=\"Index chord/mel\"):\n", "            try:\n", "                y, sr = librosa.load(fpath, sr=CONFIG[\"audio\"][\"sample_rate\"])\n", "\n", "                # Extract mel spectrogram\n", "                mel = librosa.feature.melspectrogram(\n", "                    y=y,\n", "                    sr=sr,\n", "                    n_fft=CONFIG[\"audio\"][\"n_fft\"],\n", "                    hop_length=CONFIG[\"audio\"][\"hop_length\"],\n", "                    n_mels=CONFIG[\"audio\"][\"n_mels\"],\n", "                    fmin=CONFIG[\"audio\"][\"fmin\"],\n", "                    fmax=CONFIG[\"audio\"][\"fmax\"],\n", "                )\n", "                mel_db = librosa.power_to_db(mel, ref=np.max)\n", "\n", "                # Extract chroma and chords\n", "                chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n", "                chords = self._extract_chords(chroma)\n", "                chord_indices = [\n", "                    self.chord_vocab.get(c, self.chord_vocab[\"<unk>\"])\n", "                    for c in chords\n", "                ]\n", "\n", "                seg_length = CONFIG[\"audio\"][\"max_frames\"]\n", "                mel_segments = self._segment_mel(mel_db, seg_length)\n", "                chord_segments = self._segment_chords(\n", "                    chord_indices, seg_length, len(mel_segments)\n", "                )\n", "\n", "                for mel_seg, chord_seg in zip(mel_segments, chord_segments):\n", "                    self.data.append(\n", "                        {\n", "                            \"mel\": torch.FloatTensor(mel_seg),\n", "                            \"chord\": torch.LongTensor(chord_seg),\n", "                            \"file_path\": fpath,\n", "                        }\n", "                    )\n", "\n", "            except Exception as e:  # pragma: no cover - dataset errors\n", "                print(f\"Error processing {fpath}: {e}\")\n", "                continue\n", "\n", "    def _extract_chords(self, chroma):\n", "        \"\"\"Extract chord names from chroma using music21\"\"\"\n", "        chords = []\n", "        for i in range(chroma.shape[1]):\n", "            frame = chroma[:, i]\n", "            if np.max(frame) < 0.1:  # silence threshold\n", "                chords.append(\"N\")\n", "                continue\n", "\n", "            prominent = np.where(frame > 0.5 * np.max(frame))[0]\n", "            if len(prominent) == 0:\n", "                chords.append(\"N\")\n", "                continue\n", "\n", "            try:\n", "                pitches = [m21pitch.Pitch(pitchClass=p).name for p in prominent]\n", "                m21_chord = m21chord.Chord(pitches)\n", "                chord_name = m21_chord.root().name + m21_chord.quality\n", "                chords.append(chord_name)\n", "            except Exception:\n", "                chords.append(\"N\")\n", "\n", "        return chords\n", "\n", "    def _segment_mel(self, mel, seg_length):\n", "        \"\"\"Split mel into fixed-length segments and transpose to (frames, mels)\"\"\"\n", "        segments = []\n", "        n_frames = mel.shape[1]\n", "\n", "        for i in range(0, n_frames, seg_length):\n", "            seg = mel[:, i : i + seg_length]\n", "            if seg.shape[1] < seg_length:\n", "                pad_width = seg_length - seg.shape[1]\n", "                seg = np.pad(seg, ((0, 0), (0, pad_width)), mode=\"constant\")\n", "            segments.append(seg.T)  # (frames, mels)\n", "\n", "        return segments\n", "\n", "    def _segment_chords(self, chords, seg_length, num_segments):\n", "        \"\"\"Split chords into segments aligned with mel segments\"\"\"\n", "        segments = []\n", "        for i in range(0, len(chords), seg_length):\n", "            seg = chords[i : i + seg_length]\n", "            if len(seg) < seg_length:\n", "                seg += [self.chord_vocab[\"N\"]] * (seg_length - len(seg))\n", "            segments.append(seg)\n", "\n", "        while len(segments) < num_segments:\n", "            segments.append([self.chord_vocab[\"N\"]] * seg_length)\n", "        return segments[:num_segments]\n", "\n", "    def __len__(self):\n", "        return len(self.data)\n", "\n", "    def __getitem__(self, idx):\n", "        return self.data[idx]\n", "\n", "\n", "def chord_collate(batch):\n", "    \"\"\"Collate function for chord-conditioned batches\"\"\"\n", "    mels = torch.stack([item[\"mel\"] for item in batch])\n", "    chords = torch.stack([item[\"chord\"] for item in batch])\n", "    file_paths = [item[\"file_path\"] for item in batch]\n", "\n", "    return {\"mel\": mels, \"chord\": chords, \"file_path\": file_paths}\n", "# ---- 4) Model Architecture ----\n", "class TransformerVAE(nn.Module):\n", "    def __init__(self, input_dim, chord_vocab_size, latent_dim=256):\n", "        super().__init__()\n", "        self.latent_dim = latent_dim\n", "\n", "        self.chord_embedding = nn.Embedding(chord_vocab_size, latent_dim)\n", "\n", "        self.encoder = nn.Sequential(\n", "            nn.Linear(input_dim + latent_dim, 512),\n", "            nn.ReLU(),\n", "            nn.Linear(512, 512),\n", "            nn.ReLU(),\n", "            nn.Linear(512, 2 * latent_dim),\n", "        )\n", "\n", "        self.decoder = nn.Sequential(\n", "            nn.Linear(latent_dim + latent_dim, 512),\n", "            nn.ReLU(),\n", "            nn.Linear(512, 512),\n", "            nn.ReLU(),\n", "            nn.Linear(512, input_dim),\n", "        )\n", "\n", "    def encode(self, x, chord_emb):\n", "        conditioned_x = torch.cat([x, chord_emb], dim=-1)\n", "        params = self.encoder(conditioned_x)\n", "        mu, logvar = params.chunk(2, dim=-1)\n", "        return mu, logvar\n", "\n", "    def reparameterize(self, mu, logvar):\n", "        std = torch.exp(0.5 * logvar)\n", "        eps = torch.randn_like(std)\n", "        return mu + eps * std\n", "\n", "    def decode(self, z, chord_emb):\n", "        conditioned_z = torch.cat([z, chord_emb], dim=-1)\n", "        return self.decoder(conditioned_z)\n", "\n", "    def forward(self, x, chord):\n", "        chord_emb = self.chord_embedding(chord)\n", "\n", "        batch_size, seq_len, mel_dim = x.shape\n", "        x_flat = x.view(batch_size * seq_len, mel_dim)\n", "        chord_emb_flat = chord_emb.view(batch_size * seq_len, -1)\n", "\n", "        mu, logvar = self.encode(x_flat, chord_emb_flat)\n", "        z = self.reparameterize(mu, logvar)\n", "        recon = self.decode(z, chord_emb_flat)\n", "\n", "        recon = recon.view(batch_size, seq_len, mel_dim)\n", "        mu = mu.view(batch_size, seq_len, self.latent_dim)\n", "        logvar = logvar.view(batch_size, seq_len, self.latent_dim)\n", "        return recon, mu, logvar\n", "# ---- 5) Training Functions ----\n", "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n", "    recon_loss = F.mse_loss(recon_x, x, reduction=\"sum\")\n", "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n", "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n", "\n", "\n", "def train_epoch(model, dataloader, optimizer, device, beta=1.0):\n", "    model.train()\n", "    total_loss = total_recon = total_kl = 0\n", "\n", "    for batch in tqdm(dataloader, desc=\"Training\"):\n", "        mel = batch[\"mel\"].to(device)\n", "        chord = batch[\"chord\"].to(device)\n", "\n", "        optimizer.zero_grad()\n", "        recon, mu, logvar = model(mel, chord)\n", "\n", "        loss, recon_loss, kl_loss = vae_loss(recon, mel, mu, logvar, beta)\n", "        loss.backward()\n", "\n", "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"model\"][\"grad_clip\"])\n", "        optimizer.step()\n", "\n", "        total_loss += loss.item()\n", "        total_recon += recon_loss.item()\n", "        total_kl += kl_loss.item()\n", "\n", "    n = len(dataloader.dataset)\n", "    return total_loss / n, total_recon / n, total_kl / n\n", "\n", "\n", "def validate_epoch(model, dataloader, device, beta=1.0):\n", "    model.eval()\n", "    total_loss = total_recon = total_kl = 0\n", "\n", "    with torch.no_grad():\n", "        for batch in tqdm(dataloader, desc=\"Validation\"):\n", "            mel = batch[\"mel\"].to(device)\n", "            chord = batch[\"chord\"].to(device)\n", "\n", "            recon, mu, logvar = model(mel, chord)\n", "            loss, recon_loss, kl_loss = vae_loss(recon, mel, mu, logvar, beta)\n", "\n", "            total_loss += loss.item()\n", "            total_recon += recon_loss.item()\n", "            total_kl += kl_loss.item()\n", "\n", "    n = len(dataloader.dataset)\n", "    return total_loss / n, total_recon / n, total_kl / n\n", "# ---- 6) Main Function ----\n", "def main():\n", "    os.makedirs(CONFIG[\"data\"][\"wav_dir\"], exist_ok=True)\n", "    os.makedirs(CONFIG[\"data\"][\"pretrain_data\"], exist_ok=True)\n", "\n", "    print(f\"Looking for audio files in: {CONFIG['data']['wav_dir']}\")\n", "\n", "    wav_patterns = [\"*.wav\", \"*.WAV\", \"*.mp3\", \"*.MP3\", \"*.flac\", \"*.FLAC\"]\n", "    my_wavs = []\n", "    for pattern in wav_patterns:\n", "        my_wavs.extend(Path(CONFIG[\"data\"][\"wav_dir\"]).glob(f\"**/{pattern}\"))\n", "\n", "    pretrain_files = []\n", "    for pattern in wav_patterns:\n", "        pretrain_files.extend(Path(CONFIG[\"data\"][\"pretrain_data\"]).glob(f\"**/{pattern}\"))\n", "\n", "    print(f\"Found {len(my_wavs)} files in wav_dir, {len(pretrain_files)} files in pretrain_data\")\n", "\n", "    if not my_wavs and not pretrain_files:\n", "        print(\"ERROR: No audio files found!\")\n", "        print(\"Please place audio files in the directories.\")\n", "        return None, None\n", "\n", "    chord_vocab = {}\n", "    pre_ds = None\n", "\n", "    if pretrain_files:\n", "        print(\"Building pretrain dataset...\")\n", "        pre_ds = ChordedMelDataset([str(p) for p in pretrain_files], build_vocab=True)\n", "        chord_vocab = pre_ds.chord_vocab\n", "        print(f\"Pretrain dataset size: {len(pre_ds)}\")\n", "\n", "    print(\"Building fine-tuning dataset...\")\n", "    ft_ds = ChordedMelDataset(\n", "        [str(p) for p in my_wavs], chord_vocab=chord_vocab, build_vocab=not chord_vocab\n", "    )\n", "    print(f\"Fine-tuning dataset size: {len(ft_ds)}\")\n", "\n", "    if (pre_ds is None or len(pre_ds) == 0) and len(ft_ds) == 0:\n", "        print(\"ERROR: No valid training data!\")\n", "        return None, None\n", "\n", "    bs = CONFIG[\"model\"][\"batch_size\"]\n", "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "    print(f\"Using device: {device}\")\n", "\n", "    dataloaders = {}\n", "\n", "    if pre_ds and len(pre_ds) > 0:\n", "        pre_train = DataLoader(\n", "            pre_ds,\n", "            batch_size=min(bs, len(pre_ds)),\n", "            shuffle=True,\n", "            num_workers=0,\n", "            collate_fn=chord_collate,\n", "        )\n", "        pre_val = DataLoader(\n", "            pre_ds,\n", "            batch_size=min(bs, len(pre_ds)),\n", "            shuffle=False,\n", "            num_workers=0,\n", "            collate_fn=chord_collate,\n", "        )\n", "        dataloaders[\"pretrain\"] = (pre_train, pre_val)\n", "\n", "    if len(ft_ds) > 0:\n", "        ft_train = DataLoader(\n", "            ft_ds,\n", "            batch_size=min(bs, len(ft_ds)),\n", "            shuffle=True,\n", "            num_workers=0,\n", "            collate_fn=chord_collate,\n", "        )\n", "        ft_val = DataLoader(\n", "            ft_ds,\n", "            batch_size=min(bs, len(ft_ds)),\n", "            shuffle=False,\n", "            num_workers=0,\n", "            collate_fn=chord_collate,\n", "        )\n", "        dataloaders[\"finetune\"] = (ft_train, ft_val)\n", "\n", "    input_dim = CONFIG[\"audio\"][\"n_mels\"]\n", "    model = TransformerVAE(input_dim, len(chord_vocab), CONFIG[\"model\"][\"latent_dim\"])\n", "    model = model.to(device)\n", "\n", "    optimizer = torch.optim.Adam(\n", "        model.parameters(),\n", "        lr=CONFIG[\"model\"][\"init_lr\"],\n", "        weight_decay=CONFIG[\"training\"][\"weight_decay\"],\n", "    )\n", "\n", "    for phase, (train_loader, val_loader) in dataloaders.items():\n", "        print(f\"\\n=== Starting {phase} phase ===\")\n", "        num_epochs = (\n", "            CONFIG[\"model\"][\"num_epochs_pretrain\"]\n", "            if phase == \"pretrain\"\n", "            else CONFIG[\"model\"][\"num_epochs_finetune\"]\n", "        )\n", "\n", "        for epoch in range(num_epochs):\n", "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n", "            train_loss, train_recon, train_kl = train_epoch(\n", "                model, train_loader, optimizer, device\n", "            )\n", "            val_loss, val_recon, val_kl = validate_epoch(model, val_loader, device)\n", "\n", "            print(\n", "                f\"Train Loss: {train_loss:.4f} (Recon: {train_recon:.4f}, KL: {train_kl:.4f})\"\n", "            )\n", "            print(\n", "                f\"Val Loss: {val_loss:.4f} (Recon: {val_recon:.4f}, KL: {val_kl:.4f})\"\n", "            )\n", "\n", "    print(\"\\nTraining completed!\")\n", "    return model, chord_vocab\n", "\n", "\n", "# ---- 7) Run Main ----\n", "if __name__ == \"__main__\":\n", "    try:\n", "        model, chord_vocab = main()\n", "        if model is not None:\n", "            print(\"Model training successful!\")\n", "        else:\n", "            print(\"Training failed - no data available\")\n", "    except Exception as e:  # pragma: no cover - runtime errors\n", "        print(f\"Error during training: {e}\")\n", "        import traceback\n", "\n", "        traceback.print_exc()"]}, {"cell_type": "code", "execution_count": null, "id": "01c5a93a-6bb9-415e-8690-9b02633a74c6", "metadata": {}, "outputs": [], "source": ["\u2026/checkpoints/\n", "  final_cc_tvae.pth\n", "  chord_vocab.json\n", "  audio_params.json"]}, {"cell_type": "code", "execution_count": null, "id": "e27856d9-c062-4841-8adf-9b64b578af45", "metadata": {}, "outputs": [], "source": ["# =====  A) DATA STREAMING  =====\n", "def stream_wavs_from_urls(url_list, cache_dir, max_files=None):\n", "    \"\"\"\n", "    Yaln\u0131zca URL ile indirir. Her WAV'\u0131 ge\u00e7ici dosyaya yazar,\n", "    yield eder; istersen i\u015f bitince silersin ya da LRU cache yapars\u0131n.\n", "    \"\"\"\n", "    ...\n", "\n", "def build_manifest_from_hf_or_repo(url_roots) -> list[str]:\n", "    \"\"\"\n", "    Hugging Face 'resolve' URL'leri veya GitHub raw yollar\u0131ndan\n", "    .wav listesi \u00e7\u0131kar\u0131r (haz\u0131r verilecek URL'ler de olabilir).\n", "    \"\"\"\n", "    ...\n", "\n", "def iter_dataset_from_stream(manifest, tokenizer, mode, max_files=None):\n", "    \"\"\"\n", "    mode: 'pretrain' or 'finetune'\n", "    Her wav -> mel, per_frame_chords, chord_seq, event_seq\n", "    VAE ve BC/IfO i\u00e7in gerekli objeleri \u00fcretir (yield).\n", "    \"\"\"\n", "    ...\n", "\n", "# =====  B) NEXT-CHORD BC =====\n", "class BCChordModel(nn.Module):\n", "    ...  # mevcut modeli kullan\n", "def train_bc_chords(model, seqs, ...):\n", "    ...  # mevcut train_bc ile ayn\u0131 mant\u0131k\n", "def predict_next_chord(model, context_ids, topk=1):\n", "    ...\n", "\n", "# =====  C) RL / PPO =====\n", "class SequenceEnv(gym.Env):\n", "    \"\"\"\n", "    mode: 'events' veya 'chords'\n", "    - events: action = event token id\n", "    - chords: action = chord id (next-chord policy)\n", "    reward = w1*novelty_ngram + w2*consonance + w3*style_bc (+ w4*critic) - w5*KL_ngram\n", "    \"\"\"\n", "    ...\n", "\n", "def compute_ngram_stats(seqs, n=4):\n", "    \"\"\" ger\u00e7ek korpus n-gram da\u011f\u0131l\u0131m\u0131 \"\"\"\n", "    ...\n", "\n", "def ngram_kl_penalty(gen_counts, ref_counts, eps=1e-8):\n", "    \"\"\" KL(gen||ref) \"\"\"\n", "    ...\n", "\n", "class TinyCritic(nn.Module):\n", "    \"\"\"\n", "    Hafif MLP: girdi -> (ngram histogram / token embed avg / basit \u00f6zellikler)\n", "    \u00e7\u0131kt\u0131: realism score [0,1]\n", "    \"\"\"\n", "    ...\n", "\n", "def train_tiny_critic(real_seqs, fake_seqs):\n", "    ...\n", "\n", "# =====  D) INFERENCE ENDPOINT HELPERS =====\n", "def vae_generate_from_chord_seq(model, chord_seq_ids, audio_params):\n", "    \"\"\"\n", "    Akor sekans\u0131 ver, frame'lere yay (equal split) ve VAE ile \u00fcret.\n", "    \"\"\"\n", "    ...\n"]}, {"cell_type": "code", "metadata": {}, "source": "# %% Module Imports\nfrom src.utils.endpoints import generate_accompaniment\nfrom src.rl.train import train_vfo_value\nfrom src.utils.mode_router import route_generation\nimport numpy as np\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "# %% Mini Tests\n# Generate accompaniment\nout_path = generate_accompaniment('dummy.wav')\nprint('accompaniment saved to', out_path)\n\n# VFO overfit test\nstates = np.array([0.,1.,2.,3.])\nlabels = np.array([0.,1.,1.,0.])\nmodel = train_vfo_value(states, labels)\nvalues = model(states)\nprint('values', values)\nprint('route', route_generation(True, None, {'modes':{'fallback_if_vocal_missing':True}})[0])\n", "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.4"}}, "nbformat": 4, "nbformat_minor": 5}